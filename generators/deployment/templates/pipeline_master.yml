---
stages:
- name: Build Stage
  inputs:
  - type: git
    branch: master
    service: ${REPO}
  {{#if config.triggersType}}
  triggers:
  - type: {{config.triggersType}}
  {{/if}}
  {{#has deployment.type 'Kube'}}
  properties:
  - name: CHART_NAME
    value: ${CHART_NAME}
    type: text
  {{/has}}
  jobs:
  - name: Build
    type: builder
    {{#missing deployment.type 'Kube'}}
    {{#each config.buildJobProps}}
    {{@key}}: {{{this}}}
    {{/each}}
    {{/missing}}
    {{#has deployment.type 'Kube'}}
    build_type: cr
    artifact_dir: ''
    target:
      region_id: ${REGION_ID}
      api_key: ${API_KEY}
    namespace: ${REGISTRY_NAMESPACE}
    image_name: ${CHART_NAME}
    script: |-
      #!/bin/bash
      {{#if config.javaBuildScriptContent}}
      echo "Doing Java build"
      echo "DevOps sets LOG_DIR env var, which is used during Java build to check if server started, so unsetting"
      echo "LOG_DIR: ${LOG_DIR}"
      LOG_DIR_TEMP=$LOG_DIR
      unset LOG_DIR
      echo "LOG_DIR has been unset: ${LOG_DIR}"

      {{{config.javaBuildScriptContent}}}

      echo "Java build finished, setting LOG_DIR back to original"
      LOG_DIR=$LOG_DIR_TEMP
      echo "LOG_DIR: ${LOG_DIR}"
      {{/if}}
      {{#if config.swift}}
      echo "Modifying Dockerfiles for Swift Kubernetes DevOps Toolchain deployment"
      echo "RUN cd /swift-project && /swift-utils/tools-utils.sh build release" >> Dockerfile-tools
      sed -i 's/COPY.*/COPY --from=0 \/swift-project .\/swift-project/' Dockerfile
      cat Dockerfile >> Dockerfile-tools
      mv Dockerfile-tools Dockerfile
      {{/if}}

      echo "source the container_build script to run in current shell"
      SCRIPTS_DIR={{{deployment.scriptsDir}}} # SCRIPTS_DIR is used in container_build.sh
      source ${SCRIPTS_DIR}/container_build.sh

      if  [[ -f post_build.sh ]]; then
        chmod +x post_build.sh;
        echo "executing the post_build script";
        sh post_build.sh;
      else
        echo "the post_build script does not exist";
      fi
    {{/has}}
- name: Deploy Stage
  inputs:
  - type: job
    stage: Build Stage
    job: Build
    services: HERE
    {{#if services}}
    services2: HERE
    {{/if ~}}
    {{#if this.services}}
    services3: HERE
    {{/if ~}}
  {{#has deployment.type 'Kube'}}
  properties:
  - name: buildProperties
    value: build.properties
    type: file
  - name: CHART_NAME
    value: ${CHART_NAME}
    type: text
  - name: CLUSTER_NAMESPACE
    value: ${CLUSTER_NAMESPACE}
    type: text
  - name: IMAGE_REGISTRY_TOKEN
    value: ${IMAGE_REGISTRY_TOKEN}
    type: text
  {{#if this.services }}
  {{#each this.services}}
  - name: {{{this}}}
    value: ${ '{{{this}}}' }
    type: text
  {{/each}}
  {{/if ~}}
  {{/has}}
  triggers:
  - type: stage
  jobs:
  - name: Deploy
    type: deployer
    target:
      region_id: ${REGION_ID}
      {{#has deployment.type 'CF'}}
      organization: ${CF_ORGANIZATION}
      space: ${CF_SPACE}
      {{/has}}
      application: ${CF_APP}
      api_key: ${API_KEY}
    {{#has deployment.type 'Kube'}}
      kubernetes_cluster: ${KUBE_CLUSTER_NAME}
    script: |-
      #!/bin/bash
      echo "source the kube_deploy script to run in current shell"
      source {{{deployment.scriptsDir}}}/kube_deploy.sh
      echo "export IP_ADDR=${IP_ADDR}" >> kube_vars.sh
      echo "export PORT=${PORT}" >> kube_vars.sh
    {{/has}}
    {{#has deployment.type 'CF'}}
    script: |-
      #!/bin/bash
      {{#if config.pushCommand}}
      {{{config.pushCommand}}}
      {{else}}
      cf push "${CF_APP}"
      {{/if}}
      # cf logs "${CF_APP}" --recent
    {{/has}}
- name: Health Stage
  inputs:
  - type: job
    stage: Build Stage
    job: Build
  triggers:
  - type: stage
  permission:
    execute: TOOLCHAIN_ADMINS
  {{#has deployment.type 'Kube'}}
  properties:
  - name: CLUSTER_NAMESPACE
    value: ${CLUSTER_NAMESPACE}
    type: text
  - name: IMAGE_REGISTRY_TOKEN
    value: ${IMAGE_REGISTRY_TOKEN}
    type: text
  - name: buildProperties
    value: build.properties
    type: file
  jobs:
  - name: Deploy
    type: deployer
    deploy_type: kubernetes
    target:
      region_id: ${REGION_ID}
      api_key: ${API_KEY}
      kubernetes_cluster: ${KUBE_CLUSTER_NAME}
    script: |-
      IP_ADDR=$(bx cs workers ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep normal | head -n 1 | awk '{ print $2 }')
      if [[ "${CLUSTER_NAMESPACE}" != "default" ]]; then
        RELEASE_NAME="${CLUSTER_NAMESPACE}-${IMAGE_NAME}"
      else
        RELEASE_NAME=${IMAGE_NAME}
      fi
      echo "RELEASE_NAME: $RELEASE_NAME"

      PORT=$(kubectl get services --namespace ${CLUSTER_NAMESPACE} | grep ${RELEASE_NAME} | sed 's/[^:]*:\([0-9]*\).*/\1/g')
      if [ "$(curl -Is http://$IP_ADDR:$PORT/health --connect-timeout 3 --max-time 5 --retry 2 --retry-max-time 30 | head -n 1 | grep 200)" != "" ]; then
        echo "Successfully reached health endpoint"
        echo "====================================================================="
      else
        echo "Could not reach health endpoint: http://$IP_ADDR:$PORT/health"
        exit 1;
      fi;

  {{/has}}
  {{#has deployment.type 'CF'}}
  jobs:
  - name: Test
    type: tester
    script: |-
      #!/bin/sh
      apk add --no-cache curl
      if [ "$(curl -Is http://{{manifest.host}}.{{manifest.domain}}/health  --connect-timeout 3 --max-time 5 --retry 3 --retry-max-time 30 | head -n 1 | grep 200)" != "" ]; then
        echo "Successfully reached health endpoint"
        echo "====================================================================="
      else
        echo "Could not reach health endpoint: http://{{manifest.host}}.{{manifest.domain}}/health"
        exit 1;
      fi;
    test_type: customimage
    docker_image: alpine
  {{/has}}
